---
title: "Predictive Modeling and Linear Mode"
output: html_notebook
---

## A Simple Model

Load packages. modelr package wraps around base R’s modelling functions to make them work naturally in a pipe
```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```


Two continuous variables, x and y. You can see a strong pattern in the data. 

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

Let’s use a model to capture that pattern and make it explicit. It’s our job to supply the basic form of the model. In this case, the relationship looks linear, i.e. y = a_0 + a_1 * x

Let’s start by getting a feel for what models from that family look like by randomly generating a few and overlaying them on the data. 

We'll use geom_abline() which takes a slope and intercept as parameters. 

```{r}
models <- tibble(
  a1 = runif(250, -20, 40), # intercept
  a2 = runif(250, -5, 5) # slope
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

There are 250 models on this plot, but a lot are really bad! We need to find the good models by making precise our intuition that a good model is “close” to the data. We need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of a_0 and a_1 that generate the model with the smallest distance from this data.
One easy place to start is to find the vertical distance between each point and the model...

Turn model family into a function, in order to compute distance between the y value given by the model (the prediction), and the actual y value in the data (the response).

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}
model1(c(7, 1.5), sim1)
```

Create function to get a single number, the overall distance between the predicted and actual values, using a common "root-mean-squared deviation" method. We compute the difference between actual and predicted, square them, average them, and the take the square root.

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)

```

We can use purrr to compute the distance for all the models defined above. We need a helper function because our distance function expects the model as a numeric vector of length 2.

```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

Overlay top 10 models onto the data. `-dist` color codes the models, where the ones with the smallest distance are the brightest.

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

We can also think about these models as observations, and visualising with a scatterplot of a1 vs a2, again coloured by -dist.

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

```

Next, we can do what's called a grid search. Essentially, we just generate an evenly spaced grid of points. I picked the parameters of the grid roughly by looking at where the best models were in the plot above.

```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

Now overlaying these 10 models back on the original data...

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

To narrow in on the best model, we could use a numerical minimisation tool called Newton-Raphson search. The intuition of Newton-Raphson is pretty simple: you pick a starting point and look around for the steepest slope. You then go down that slope a little way, and then repeat again and again, until you can’t go any lower. In R, we can do that with optim():

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par # gives back a vector: [intercept, slope]
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

There’s one more approach that we can use for this model, because it’s is a special case of a broader family: linear models. A linear model has the general form `y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n - 1)`. So this simple model is equivalent to a general linear model where n is 2 and x_1 is x. R has a tool specifically designed for fitting linear models called `lm()`. `lm()` has a special way to specify the model family: formulas. Formulas look like `y ~ x`, which `lm()` will translate to a function like `y = a_1 + a_2 * x`. We can fit the model and look at the output:

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

`lm()` actually finds the closest model in a single step. This approach is both faster, and guarantees that there is a global minimum.

## Visualizing Models

To visualise the predictions from a model, we can use modelr::data_grid() to generate an evenly spaced grid of values that covers the region where data lies. Its first argument is a data frame, and for each subsequent argument it finds the unique variables and then generates all combinations:

```{r}
grid <- sim1 %>% 
  data_grid(x) 
grid
```

Next, we add predictions with `modelr::add_predictions()`, which takes a data frame and a model.

```{r}
grid <- grid %>% 
  add_predictions(sim1_mod) 
grid
```


Then we plot the predictions. The advantage of this approach over `geom_abline()` is that it will work with any model in R. 

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)
```

## Residuals

The flip-side of predictions are residuals. The predictions tells you the pattern that the model has captured, and the residuals tell you what the model has missed. The residuals are just the distances between the observed and predicted values that we computed above.
We add residuals to the data with `add_residuals()`, which works much like `add_predictions()`. Note, however, that we use the original dataset, not a manufactured grid. This is because to compute residuals we need actual y values.

```{r}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)
sim1
```

We can draw a frequency polygon to help us understand the spread of the residuals. This helps us calibrate the quality of the model: how far away are the predictions from the observed values? Note that the average of the residual will always be 0.

```{r}
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```

We'll often want to recreate plots using the residuals. The following plot looks like random noise. Seemingly random residuals indicate the model is pretty good.

```{r}
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point()
```

## Formulas and Model Families

The majority of modelling functions in R use a standard conversion from formulas to functions. For example, `y ~ x` is translated to `y = a_1 + a_2 * x`. `model_matrix()` shows what R actually does. It takes a data frame and a formula and returns a tibble that defines the model equation: each column in the output is associated with one coefficient in the model, the function is always `y = a_1 * out1 + a_2 * out_2`. The following illustrates `y ~ x1`

```{r}
df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)
model_matrix(df, y ~ x1)
```


The model matrix grows in an unsurprising way when you add more variables to the the model:

```{r}
model_matrix(df, y ~ x1 + x2)
```

This formula notation is sometimes called “Wilkinson-Rogers notation”, and was initially described in Symbolic Description of Factorial Models for Analysis of Variance, by G. N. Wilkinson and C. E. Rogers https://www.jstor.org/stable/2346786. 

What follows will expand on how this formula notation works for categorical variables, interactions, and transformation.


## Categorical Variables

Things get a bit more complicated when the predictor is categorical...

Imagine you have a formula like `y ~ sex`, where sex could either be male or female. It doesn’t make sense to convert that to a formula like `y = x_0 + x_1 * sex` because `sex` isn’t a number - you can’t multiply it.

Instead what R does is convert it to `y = x_0 + x_1 * sex_male` where `sex_male` is one if `sex` is male and zero otherwise:

```{r}
df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1
)
model_matrix(df, response ~ sex)
```


To visualize predictions, we'll use the `sim2` dataset from modelr:

```{r}
ggplot(sim2) + 
  geom_point(aes(x, y))
```


We'll fit a model to it and generate predictions:

```{r}
mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
grid

```

Then overlay the predictions over the data:

```{r}
ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

We can see that a model with a categorical x will predict the mean value for each category. This is because the mean minimises the root-mean-squared distance. 

## Interactions (Continuous and Categorical)

We'll use `sim3` from modelr, which contains a categorical predictor and a continuous predictor.

```{r}
ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(colour = x2))

```

There are two possible models you could fit to this data:

```{r}
# Multiple linear models
mod1 <- lm(y ~ x1 + x2, data = sim3) # no interaction effect
mod2 <- lm(y ~ x1 * x2, data = sim3) # yes interaction effect
```


To visualise these models:

1) We have two predictors, so we need to give data_grid() both variables. It finds all the unique values of x1 and x2 and then generates all combinations.

2) To generate predictions from both models simultaneously, we can use gather_predictions() which adds each prediction as a row. The complement of gather_predictions() is spread_predictions()which adds each prediction to a new column.

```{r}
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
grid
```

We can visualise the results for both models on one plot using facetting and determine existence of interaction effects:

```{r}
ggplot(sim3, aes(x1, y, colour = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model)
```

If we allow for an interaction variable, and we see the slopes are different, it is an indication that there is an interaction effect.

We can take look at the residuals to see which model is better for this data.

```{r}
sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, colour = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2)

```

There is little obvious pattern in the residuals for `mod2`. The residuals for `mod1` show that the model has clearly missed some pattern in `b`, and less so, but still present is pattern in `c`, and `d`.

## Interactions (Two continuous)

We'll look at the model for two continuous variables.

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>% 
  data_grid(
    x1 = seq_range(x1, 5), 
    x2 = seq_range(x2, 5) 
  ) %>% 
  gather_predictions(mod1, mod2)
grid
```

Then visualize the model:

```{r}
ggplot(grid, aes(x1, x2)) + 
  geom_tile(aes(fill = pred)) + 
  facet_wrap(~ model)
```

Our eyes aren't really that great at discerning shades in color. 




















